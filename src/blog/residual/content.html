<main class="my-4 col-md-8 col-xl-9 col-12 text-break px-3 fs-5">
    <div class="mx-auto" style="max-width: 700px;">
        <div class="row p-3 pb-1">
            <paragraph>
                <text>
                    Residual streams [[highway]] are a key component of the transformer architecture [[transformer]] and
                    are common in deep convolutional neural networks [[resnet]]. The skip connections between layers
                    allow gradients to propagate backwards through the model efficiently. Karpathy describes residual
                    streams as a way to optimise all the lines of a short program in parallel [[karpathy]], which I like
                    a lot.
                </text>
            </paragraph>
            <fig>
                <body>
                    <visual>
                        <src>/images/residual/transformer.png</src>
                        <alt>A diagram of a residual block in the transformer architecture.</alt>
                        <title>Residual Block</title>
                    </visual>
                </body>
                <id>transformer</id>
                <caption>An illustration of a single block in the transformer architecture.</caption>
            </fig>
            <paragraph>
                <text>
                    Operations on residual streams are often characterised as reads and writes. This communication is a key component of the stream, which acts as a shared, bottlenecked memory between layers and allows modules to iteratively refine a set of shared features into more specialised ones for prediction. One downside of this approach is that small changes to parameters early in the model could be amplified in later layers and a module that reads from the stream later may expect a different distribution after the update. This problem is overcome with normalisation: batch normalisation in CNNs and layer normalisation in transformers.
                </text>
            </paragraph>
            <paragraph>
                <text>
                    Before continuing, I must address the confusing terminology as “layer” has specific meanings for each model. Each of the transformers is made up of 12 or 24 layers that contain modules. A ResNet is made up of 5 layers containing between 7 and 49 blocks, which I will refer to as modules when comparing with language models. On the charts, I plot progression in layers in transformers and progression in blocks for the ResNets with vertical lines showing the separation between ResNet layers. 
                </text>
            </paragraph>
            <paragraph>
                <text>
                    Another piece of confusing terminology is “features”. Features are properties of the data (dog ears, circles, curves, the texture of broccoli, etc) and some features may not appear in a certain dataset (there are no dog ears in MNIST). Activations are properties of the model when certain data is passed through it - they occur after an operation (add, ReLU, attention, etc) and can be viewed at different resolutions (per neuron, channel, etc). 
                </text>
            </paragraph>
            <paragraph>
                <text>
                    Starting with an up-close inspection of image networks, I want to highlight some neuron activations seen under the OpenAI Microscope [[microscope]]. The initial visualisation is channel 1594 of layer 4 block 3 from ResNet-50 [(fig-feature)]. The choice is fairly arbitrary but is part of some exploration I was doing on the model. From the weights of the last layer, we can see that this channel has the highest contribution to the final probability for class 1 (goldfish). Backtracking through the blocks In the final layer, this feature has been refined into a more and more goldfish-like feature until it makes the final classification (see this notebook for more details).
                </text>
            </paragraph>
        </div>
    </div>
    <fig>
        <body>
            <div id="visualisation" class="mx-auto">
                <div class="form-group mx-auto py-3" style="max-width: 250px;">
                    <div class="input-group w-auto">
                        <span class="input-group-text" id="feature-description">Feature number: </span>
                        <input type="number" class="form-control text-center" id="feature-input" min="0" max="2047" value="1594"
                            aria-describedby="feature-description feature-help">
                    </div>
                    <div class="form-text text-center" id="feature-help">Enter the id of the feature to visualize (0-2047).
                    </div>
                </div>
                <div class="row">
                    <feature>
                        <channel>1594</channel>
                        <unit>3</unit>
                    </feature>
                    <feature>
                        <channel>1594</channel>
                        <unit>2</unit>
                    </feature>
                    <feature>
                        <channel>1594</channel>
                        <unit>1</unit>
                    </feature>
                </div>
                <ul class="nav flex-row text-center mx-auto justify-content-center pb-4">
                    <explanation />
                    <explanation />
                    <explanation />
                    <explanation />
                    <explanation />
                </ul>
            </div>
            <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.0/jquery.min.js"></script>
            <script src="/static/imagenet-1k-classes.js"></script>
            <script src="/static/resnet-50-weights.js"></script>
            <script>
                function update_visualisation() {
                    let feature = $("#visualisation input").val();
                    if (feature === "" || isNaN(feature) || feature < 0 || feature > 2047) {
                        return;
                    }
                    $("#visualisation img").each(
                        (i, e) => $(e).attr("src", `https://openaipublic.blob.core.windows.net/microscopeprod/2020-07-25/2020-07-25/resnetv2_50_slim/lucid.dataset_examples/_dataset_examples/dataset%3Dimagenet%26op%3Dresnet_v2_50%252Fblock4%252Funit_${3 - i}%252Fbottleneck_v2%252Fadd%253A0/channel_${feature}_40.png`)
                    )
                    $("#visualisation .explanation").each(
                        function (i, e) {
                            let class_weight = weights[$("#visualisation input").val()][i];
                            let class_name = classes[class_weight[0]];
                            let weight = class_weight[1];
                            let class_weight_str = (weight >= 0 ? "+" : "") + weight.toFixed(2);
                            $(e).text(`${class_name} (${class_weight_str})`);

                        }
                    )
                }
                $(document).ready(update_visualisation)
                $("#visualisation input").on("input", update_visualisation)
            </script>
        </body>
        <id>fig-visualisation</id>
        <caption>An interactive visualisation of the final layer of a ResNet-50. Each image contains samples from the dataset that cause a high activation in the given channel. The weights below give the contribution of this feature to each of the classes before the softmax and final prediction.</caption>
    </fig>
    <div class="mx-auto" style="max-width: 700px;">
        <div class="row p-3 pb-1">
            <paragraph>
                <text>
                    This is my first attempt at an interactive visualisation [(fig-visualisation)] in a blog post so it's fairly primitive but I encourage you to visualise different channels so that you have more intuition about the results later. All of the remaining charts give layer-by-layer overviews, however, the visualisations are a useful part of interpreting and understanding neural networks, especially when combined with other approaches [[visualisation]].
                </text>
            </paragraph>
        </div>
    </div>
    <div class="mx-auto" style="max-width: 700px;">
        <bibliography />
    </div>
</main>