<h1 class="py-5 px-3 text-center">Image Classifier Attacks</h1>
<section>
    <id>introduction</id>
    <header>Introduction</header>

    <body>
        <paragraph>
            <text>Image classifiers have achieved incredibly high accuracy in classifying images[[alexnet]]. Still,
                no model is perfect and image space is so incredibly large that we can expect pockets in which we can
                find incorrectly-classified images. Additionally, we can anticipate that there are some areas of image
                space not represented in datasets that the classifiers
                do not agree on. Here, we exploit this by creating fake images that are very close to real images in
                image space but far enough away that they are misclassified.
            </text>
        </paragraph>

    </body>
</section>
<section>
    <id>background</id>
    <header>Background</header>

    <body>
        <paragraph><text>Neural networks work best with data that follows a standard normal distribution. Images
                generally do not obey this but a series of transforms allows them to[[alexnet]]. We exploit this
                feature by engineering spiky noise that “confuses” the classifier, leading to an incorrect label. To
                make this even more obvious, we decide on an alternative class and try to move the image towards a
                point that minimises the loss on this class while leaving the image visually very similar.
            </text></paragraph>
        <paragraph><text>Classifier attacks are particularly harmful against public APIs, particularly when open
                source models have been used and approaches like the one outlined here can be applied. However,
                printing images and holding them up to cameras can also be effective and approaches like these may
                seem suspicious but the intentions of fooling a classifier may not be obvious to passers by.</text>
        </paragraph>
        <paragraph><text>In 2018, researchers demonstrated that they could trick the object recognition system in
                self-driving cars to misclasify stop signs as speed-limit signs. The stickers placed on them looked
                like random graffiti to passers-by, but had a much greater affect on the system inside the
                cars[[stop]].
            </text></paragraph>
    </body>
</section>
<section>
    <id>approach</id>
    <header>Approach</header>

    <body>
        <p class="py-3">The aim of a neural network is to minimise a loss function by gradient descent:
        <p>$$\theta_{n+1} = \theta_n - \alpha \frac{\partial \mathcal L}{\partial \theta_n}$$
        <p>We make two simple changes to the parameters of this equation to achieve the desired result:
        <ol class="px-5">
            <li class="py-1">Let \(\theta_n\) parameterise the image - not the classifier
            <li class="py-1">Change the loss to be $$\mathcal {\hat L} =\frac{1}{N} \sum_{i=0}^{N-1} w_i\
                BCE(f_i(\theta), T_i)$$ where \(f_i\)
                is the \(i^{th}\) classifier, \(T_i\) is the target class and \(w_i\) is the weight given to
                the \(i^{th}\) classifier. This is a weighted binary cross-entropy loss.
        </ol>
        <p>We also modify the loss for faster convergence by adding a normalisation term to the loss and setting
            $$w_i = \begin{cases} 1 & \text{if } \text{confidence}_i \ge 95\% \\ 0 & \text{otherwise}
            \end{cases}$$
        <div class="d-flex-col justify-content-start">
            <div class="py-2">
                <h3>Models</h3>
                <p class="py-3">For this experiment, the models we used were:
                <ul class="px-5">
                    <li>AlexNet[[alexnet]]
                    <li>DenseNet-121[[densenet]]
                    <li>EfficientNet-b0[[efficientnet]]
                    <li>MobileNet-v2[[mobilenet]]
                    <li>ResNet-50[[resnet]]
                    <li>SqueezeNet-v1[[squeezenet]]
                    <li>VGG-11[[vgg]]
                </ul>
                <p>All of these models were all trained on ImageNet and are available with pretrained weights on
                    the <a href="https://pytorch.org/vision/stable/models.html#classification">PyTorch Hub</a>
            </div>
        </div>
        </div>
    </body>
</section>
<section>
    <id>results</id>
    <header>Results</header>

    <body>
        <paragraph><text>This trick works incredibly well - giving images a fine layer of noise that confuses
                classifiers and achieves the desired incorrect predictions to a very high level of accuracy.</text>
        </paragraph>
        <div class="container">
            <div class="row flex-row d-flex justify-content-evenly py-4">
                <div class="d-flex flex-column flex-align-center w-auto p-1">
                    <h3 class="text-center p-2">Apple</h3>
                    <img src="/images/classifier-attack/apple.bmp" alt="Granny Smith" title="Granny Smith"
                        class="img-fluid mb-3 m-auto p-3">
                </div>
                <div class="w-auto p-1">
                    <table class="table table-striped table-responsive-lg" style="max-width:400px">
                        <thead>
                            <tr>
                                <th scope="col">Model</th>
                                <th scope="col">Class</th>
                                <th scope="col">Score</th>
                            </tr>
                        </thead>
                        <tbody class="table-group-divider">
                            <tr>
                                <td>AlexNet</td>
                                <td>pomegranate</td>
                                <td>89.4%</td>
                            </tr>
                            <tr>
                                <td>DenseNet-121</td>
                                <td>pineapple</td>
                                <td>97.5%</td>
                            </tr>
                            <tr>
                                <td>EfficientNet-b0</td>
                                <td>strawberry</td>
                                <td>90.2%</td>
                            </tr>
                            <tr>
                                <td>Mobilenet-v3</td>
                                <td>orange</td>
                                <td>94.1%</td>
                            </tr>
                            <tr>
                                <td>ResNet-50</td>
                                <td>jackfruit</td>
                                <td>91.6%</td>
                            </tr>
                            <tr>
                                <td>SqueezeNet-v1.1</td>
                                <td>lemon</td>
                                <td>98.2%</td>
                            </tr>
                            <tr>
                                <td>VGG-11</td>
                                <td>banana</td>
                                <td>96.1%</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <div class="row flex-row-reverse d-flex justify-content-evenly py-4">
                <div class="d-flex flex-column flex-align-center w-auto p-1">
                    <h3 class="text-center p-2">Ball</h3>
                    <img src="/images/classifier-attack/ball.bmp" alt="Soccer Ball" title="Soccer Ball"
                        class="img-fluid mb-3 m-auto p-3">
                </div>

                <div class="w-auto p-1">
                    <table class="table table-striped table-responsive-lg" style="max-width:400px">
                        <thead>
                            <tr>
                                <th scope="col">Name</th>
                                <th scope="col">Class</th>
                                <th scope="col">Score</th>
                            </tr>
                        </thead>
                        <tbody class="table-group-divider">
                            <tr>
                                <td>AlexNet</td>
                                <td>croquet ball</td>
                                <td>90.3%</td>
                            </tr>
                            <tr>
                                <td>DenseNet-121</td>
                                <td>ping-pong ball</td>
                                <td>98.4%</td>
                            </tr>
                            <tr>
                                <td>EfficientNet-b0</td>
                                <td>baseball</td>
                                <td>98.6%</td>
                            </tr>
                            <tr>
                                <td>Mobilenet-v3</td>
                                <td>basketball</td>
                                <td>98.9%</td>
                            </tr>
                            <tr>
                                <td>ResNet-50</td>
                                <td>tennis ball</td>
                                <td>97.6%</td>
                            </tr>
                            <tr>
                                <td>SqueezeNet-v1.1</td>
                                <td>golf ball</td>
                                <td>98.9%</td>
                            </tr>
                            <tr>
                                <td>VGG-11</td>
                                <td>volleyball</td>
                                <td>98.1%</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>


            <div class="row flex-row d-flex justify-content-evenly py-4">
                <div class="d-flex flex-column flex-align-center w-auto p-1">
                    <h3 class="text-center p-2">Pig</h3>
                    <img src="/images/classifier-attack/pig.bmp" alt="Hog" title="Hog" class="img-fluid"
                        class="img-fluid mb-3 m-auto p-3">
                </div>
                <div class="w-auto p-3">
                    <table class="table table-striped table-responsive-lg" style="max-width:400px">
                        <thead>
                            <tr>
                                <th>Name</th>
                                <th>Class</th>
                                <th>Score</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>AlexNet</td>
                                <td>brown bear</td>
                                <td>89.5%</td>
                            </tr>
                            <tr>
                                <td>DenseNet-121</td>
                                <td>cougar</td>
                                <td>98.6%</td>
                            </tr>
                            <tr>
                                <td>EfficientNet-b0</td>
                                <td>zebra</td>
                                <td>96.4%</td>
                            </tr>
                            <tr>
                                <td>Mobilenet-v3</td>
                                <td>tiger</td>
                                <td>97.7%</td>
                            </tr>
                            <tr>
                                <td>ResNet-50</td>
                                <td>triceratops</td>
                                <td>94.4%</td>
                            </tr>
                            <tr>
                                <td>SqueezeNet-v1.1</td>
                                <td>hippopotamus</td>
                                <td>98.9%</td>
                            </tr>
                            <tr>
                                <td>VGG-11</td>
                                <td>Komodo dragon</td>
                                <td>99.0%</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </body>
</section>
<section>
    <id>limitations</id>
    <header>Limitations</header>

    <body>
        <div class="py-2">
            <h3>Brittleness</h3>
            <paragraph><text>Even though the predictions are very poor on the fake images, these images are
                    very brittle. Smoothing them, resizing them or even saving them as a JPG or PNG (which
                    removes most of the noise during compression) results in almost identical classification to
                    the original images.</text></paragraph>
        </div>
        <div class="py-2">
            <h3>Resources</h3>
            <paragraph><text>This process is time-consuming, taking ~6 hours to run on a single laptop GPU
                    (one model loaded at a time). While it can be parallelised for many images and sped up
                    dramatically by leaving all models on the same GPU, this requires more computing resources.</text>
            </paragraph>
        </div>
    </body>
</section>
<section>
    <id>conclusions</id>
    <header>Conclusions</header>

    <body>
        <paragraph><text>Modern image classifiers are extremely accurate and robust. While the approach presented
                here manages to create images that are classified incorrectly, the effort and quality of results
                demonstrate the strength of these models rather than displaying a potential weakness. As a result,
                there is limited scope for how this could be used maliciously as routine transforms correct most of
                the damage.</text></paragraph>
    </body>
</section>
<bibliography />
<github>
    <repo>https://github.com/George-Ogden/classifier-attack</repo>
</github>