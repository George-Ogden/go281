<h1 class="py-5 px-3 text-center">PPO Research</h1>
<section>
    <id>introduction</id>
    <header>Introduction</header>

    <body>
        <paragraph><text>Proximal Policy Optimisation [[PPO]] is the leading algorithm for training reinforcement
                learning
                models. Like other reinforcement learning algorithms, it suffers from data inefficiency and converges to
                local instead of global optima. In this post, I present some new research that improves the efficiency
                and performance of the algorithm with some tricks.
            </text>
        </paragraph>
    </body>
</section>
<section>
    <id>updates</id>
    <header>Update Ordering</header>

    <body>
        <paragraph>
            <text>
                The first trick applies to general actor-critic setups, however, I have tested the code on PPO to
                compare with SOTA results. The PPO algorithm [[PPO]] is as follows:
            </text>
        </paragraph>
        <blockquote>
            <b>for</b> iteration \(i= 1, 2, \ldots\) <b>do</b><br>
            &emsp;<b>for</b> actor \(= 1, 2, \ldots, N \) <b>do</b><br>
            &emsp;&emsp;Run policy \(\pi_{\theta_{i-1}}\) in environment for \(T\) timesteps<br>
            &emsp;&emsp;Compute advantage estimates \(\hat{A}_t = G(s_t,a_t) - V_{\theta_{i-1}}(s_t)\)<br>
            &emsp;<b>end for</b><br>
            &emsp;Optimize surrogate \(L\) wrt \( \theta_i \), with \(K\) epochs and minibatch size \(M\)<br>
            <b>end for</b>
        </blockquote>
        <paragraph>
            <text>
                The main problem with the current algorithm is the way that the advantage \(\hat{A}_t\) is calculated.
                When calculating the advantage at iteration, \(i\), we use the value network, \(V_{\theta_i}\) trained
                using experience from the policy \(\pi_{i-1}\). While the divergence of the policy is small, I propose
                using a value network, \(V_{\theta_i}\) trained using experience from the policy \(\pi_{\theta_i}\):
            </text>
        </paragraph>
        <blockquote>
            <b>for</b> iteration \(i= 1, 2, \ldots\) <b>do</b><br>
            &emsp;<b>for</b> actor \(= 1, 2, \ldots, N \) <b>do</b><br>
            &emsp;&emsp;Run policy \(\pi_{\theta_{i-1}}\) in environment for \(T\) timesteps<br>
            &emsp;<b>end for</b><br>
            &emsp;Optimize surrogate \(L_V\) wrt \( V_{\theta_i} \), with \(K\) epochs and minibatch size \(M\)<br>
            &emsp;Compute advantage estimates \(\hat{A}_t = G(s_t,a_t) - V_{\theta_i}(s_t)\)<br>
            &emsp;Optimize surrogate \(L_{\pi}\) wrt \( \pi_{\theta_i} \), with \(K\) epochs and minibatch size
            \(M\)<br>
            <b>end for</b>
        </blockquote>
        <paragraph>
            <text>
                For this and subsequent experiments, I use the same setup as [[implementation]] and we can see that my
                version improves on both Acrobot and CartPole classic control environments. The modification does not
                solve the exploration problem, hence all algorithms score 0 on MountainCar.
            </text>
        </paragraph>
        <wandb>
            <url>
                https://wandb.ai/george_ogden/ppo-critic-experiments/reports/Classic-Control-Critic-First-PPO--Vmlldzo1OTI0MTE2
            </url>
        </wandb>
        <paragraph>
            <text>
                Additionally, I modify the Spinning Up [[spinup]] implementation in the same way to see that this
                updated version performs better on Acrobot, similarly on CartPole and again fails to solve MountainCar.
            </text>
        </paragraph>
        <wandb>
            <url>
                https://wandb.ai/george_ogden/ppo-spinup-critic/reports/SpinUp-Classic-Control-Critic-First-PPO--Vmlldzo1OTMwMDM5
            </url>
        </wandb>
    </body>
</section>
<section>
    <id>support</id>
    <header>Value Support</header>

    <body>
        <paragraph>
            <text>The next critic trick is a negative result, however, this would not be science if I left it out. In <a
                    href=/blog/curling>an earlier post</a>, I described how value support was useful for improving
                performance. Unfortunately, this turns out not to be the case. The idea is to represent the critic
                network as a probability distribution over possible values (on the support) and to use the mean as the
                estimated value. For more details see <a href="/blog/curling#support-explanation">this extract from the
                    previous post</a>. This should work better because it is hard for a model to generate features
                correlated (linearly related) with a value. Here, I test this claim more rigorously, however, we see
                that there is only a marginal improvement on Acrobot, a decrease in performance on CartPole and again
                scores 0 on MountainCar.</text>
        </paragraph>
        <wandb>
            <url>https://wandb.ai/george_ogden/ppo-critic-experiments/reports/Value-Support-PPO--Vmlldzo1OTI0MzEz</url>
        </wandb>
    </body>
</section>
<section>
    <id>rnns</id>
    <header>LSTM vs GRU</header>

    <body>
        <paragraph>
            <text>
                The next experiments investigate a problem that remains misunderstood: LSTM vs GRU. With the rise of
                transformers, LSTMs and GRUs have fallen out of fashion for next-token prediction tasks. However, the
                advice I always went to when choosing between them was that no one understood which would work better on
                a given task, so try both and pick the one that does best for the problem. In RL, it is still common to
                use LSTMs for recurrent architectures. However, we can see that GRU performs just as well on BeamRider
                and Pong and even better on Breakout. On top of this, we can see that it runs slightly quicker due to
                the simpler architecture, however, an issue with the training setup has caused the Breakout LSTM
                experiment to take significantly longer.
            </text>
        </paragraph>
        <wandb>
            <url>
                https://wandb.ai/george_ogden/ppo-rnn-experiments/reports/Atari-NoFrameSkip-RNN-Architectures--Vmlldzo1OTI0NTAx
            </url>
        </wandb>
    </body>
</section>
<section>
    <id>comb</id>
    <header>Continuous Distributions</header>

    <body>
        <paragraph>
            <text>
                The final experiments challenge the assumption that you should use a Gaussian Distribution with a
                diagonal covariance matrix. Instead, I use a general distribution with pointwise estimates of the PDF at
                equally spaced intervals. The action dimensions are still independent meaning the model outputs
                \(n_\text{combs} * \text{action_dim}\) values that are normalised across each \(\text{action_dim}\).
            </text>
        </paragraph>
        <paragraph>
            <text>
                The comb distribution performs better on Cheetah but worse on Hopper and Walker. I cannot offer an
                explanation as all of these environments seem very similar in terms of action dimensionality and
                objective. Even though it allows generalisation to a wider class of distributions, this generalised distribution
                is much slower to run and more complicated to implement.
            </text>
        </paragraph>
        <wandb>
            <url>
                https://wandb.ai/george_ogden/ppo-continuous-experiments/reports/Continuous-PPO--Vmlldzo1OTMwODky
            </url>
        </wandb>
    </body>
</section>